---
author: [""]
title: "Ultra-scale Playbook - Data Parallelism" 
date: "2025-04-27"
tags: ["llm", "llm-training"]
series: ["Ultra-scale Playbook"]
description: ""
summary: "Notes on training LLMs using data parallelism strategy"
ShowToc: false
ShowBreadCrumbs: false
draft: true
---

HuggingFace released an awesome [open-source book](https://huggingface.co/spaces/nanotron/ultrascale-playbook) on training LLMs upto 12k GPUs.
